import requests
from time import sleep
import pickle
from bs4 import BeautifulSoup
import numpy as np
import pandas as pd

'''
三星终端列表网址：
https://www.phone888.cn/specs/?brand=5
https://www.phone888.cn/specs/?brand=5&z=2
https://www.phone888.cn/specs/?brand=5&z=3
……
https://www.phone888.cn/specs/?brand=5&z=68
'''


page = 68  #一共有68页


#获取所有手机的网址

list_allphone=[]  #爬取的目标手机列表页面共68页，将终端网址存放倒列表中

for i in range(page):
    list_allphone.append('https://www.phone888.cn/specs/?brand=5&z='+str(i+1))
#print(list_allphone)
list_allphone[0]='https://www.phone888.cn/specs/?brand=5'
print(list_allphone)

all_before=[] #将获取到的列表网页源码，存放到此列表中
for i in range(len(list_allphone)): #获取每一页手机列表页面的源码，后续从中提取各型号手机参数网址
    all_before.append(i)
    print(i)
    strhtml = requests.get(list_allphone[i])
    sleep(20)
    strhtml.encoding = 'utf-8'
    #print(strhtml.text)
    all_before.append(strhtml.text)

file_road = r'D:\pycharm\samsung\phone888_samsung_list'
pickle.dump(all_before,open(file_road,'wb')) #将列表打包保存到本地


#获取每一款终端的网页源码
file_road = r'D:\pycharm\samsung\phone888_samsung_list'
all_before=pickle.load(open(file_road,'rb'))
for j in range(5): #len(all_before)/2  #设置一次爬取的页面数
    num=47*2 #42、46页有问题  #由于在爬取过程中会因为各种原因中断，num设置重新开始爬取的页数
    i=num+j*2+1
    print(i)
    soup = BeautifulSoup(all_before[i], 'lxml')
    list_sony_all_source = []
    for link in soup.find_all('div', 'bl1'):
        phone_name = link.find('strong').get_text()
        list_sony_all_source.append(phone_name)
        print(phone_name)
        phone_href = 'https://www.phone888.cn' + link.find('a').get('href')
        list_sony_all_source.append(phone_href)
        print(phone_href)
        strhtml = requests.get(phone_href)
        sleep(15)
        strhtml.encoding = 'utf-8'
        list_sony_all_source.append(strhtml.text)
    file_road ='D:\pycharm\samsung\html\phone888_samsung_all_source_' + str(i)
    pickle.dump(list_sony_all_source, open(file_road, 'wb')) #每爬取一页保存一次列表
    print(file_road)





'''
#查看源码，提取规则
soup = BeautifulSoup(all_before[0], 'html.parser')
print(soup.prettify())  #按照标准的缩进格式的结构输出
for link in soup.find_all('div', 'makers'):
    for link2 in link.find_all('li'):
        print(link2.find('a').get_text())
        print('https://www.gsmarena.com'+link2.find('a').get('href'))


file_road = r'E:\pycharm\phone\gsmarena_samsung\gsmarena_samsung_before'
# pickle.dump(all_before,open(file_road,'wb'))
# print(len(all_before))
all_before = pickle.load(open(file_road, 'rb'))

# 获取所有手机的源码
k = 0  # 若爬虫中断，可以调整k值，继续爬虫
list_sony_all_source = []
for j in range(len(all_before) - k):
    i = j + k
    print(i)
    soup = BeautifulSoup(all_before[i], 'lxml')
    # list_sony_all_source = []
    for link in soup.find_all('div', 'makers'):
        for link2 in link.find_all('li'):
            phone_name = link2.find('a').get_text()  # 提取手机型号
            list_sony_all_source.append(phone_name)
            print(phone_name)
            phone_href = 'https://www.gsmarena.com/' + link2.find('a').get('href')  # 提取手机参数网址
            list_sony_all_source.append(phone_href)
            print(phone_href)
            # strhtml = requests.get(phone_href)  # 获取手机参数页面源码
            # sleep(15)
            # strhtml.encoding = 'utf-8'
            # list_sony_all_source.append(strhtml.text)
    # file_road =r'E:\pycharm\phone\gsmarena_samsung\gsmarena_samsung_source_' + str(i)
    # pickle.dump(list_sony_all_source, open(file_road, 'wb'))
    # print(file_road)

file_road = r'E:\pycharm\phone\gsmarena_samsung\gsmarena_samsung_href'
pickle.dump(list_sony_all_source, open(file_road, 'wb'))
print(len(list_sony_all_source))

#提取各型号手机的属性参数
result = pd.DataFrame([['title', 'param', 'brand', 'model']], columns=['title', 'param', 'brand', 'model'])
for j in range(page):
    # file_road=r'E:\pycharm\phone\phonemore_sony_Ericsson_source_0'
    file_road = 'E:\pycharm\phone\phonemore_sony_Ericsson_source_' + str(j)
    a1 = pickle.load(open(file_road, 'rb'))
    a2 = np.array(a1)
    all_source = a2.reshape((int(len(a2) / 3), 3))
    for i in range(len(all_source)):
        soup = BeautifulSoup(all_source[i][2], 'html.parser')
        # print(soup.prettify())  #按照标准的缩进格式的结构输出，从源码中观察参数展示规律，以便提取
        list_param = []
        for link in soup.find_all('tr'): #提取属性参数
            list_md = []
            for link1 in link.find_all('td'):
                list_md.append(link1.get_text())
                # print(link1.get_text())
            list_param.append(list_md)
        # print(list_param)
        test = pd.DataFrame(list_param, columns=['title', 'param'])
        test['brand'] = 'sony'
        test['model'] = all_source[i][0]
        # print(test)
        result = result.append(test)
result.to_csv(r"E:\pycharm\phone\phonemore_sony_Ericsson_20201029.csv",index=False,encoding='utf_8_sig')


#规格化手机属性参数
input_path=r"E:\pycharm\phone\phonemore_sony_Ericsson_20201029.csv"
name_phone=['参数名','参数','品牌','型号']
phone_all=pd.read_csv(open(input_path, 'rb'), names=name_phone,encoding = 'utf-8')
phone_xh=phone_all[['型号']]
phone_xh.drop_duplicates(inplace=True) #去重
col_xh = np.array(phone_xh).tolist()
col_xh.remove(['model']) #表中原有的字段名，删除
#print(col_xh)
col=phone_all[['参数名']]
col.drop_duplicates(inplace=True) #去重
list_col = np.array(col).tolist()
list_col.remove(['title'])  #表中原有的字段名，删除
#print(list_col)
col_re=[]
for i in range(len(list_col)):
	col_re.append(list_col[i][0])
#print(col_re)
template=pd.DataFrame(list_col,columns=['参数名']) #组合所有参数名，作为标准格式
result = pd.DataFrame(np.arange(len(col_re)).reshape(1,len(col_re)), columns=col_re) #最后的结果格式
#print(template)

for i in range(len(col_xh)):
    print(col_xh[i][0])
    phone_3 = phone_all[phone_all['型号'] == col_xh[i][0]]
    phone_4 = pd.merge(template, phone_3, on='参数名', how='left')  # 连接
    phone_5 = phone_4[['参数名', '参数']]
    phone_6 = phone_5[['参数名', '参数']].groupby(by=['参数名']).max()
    phone_6.reset_index(inplace=True)
    phone_7 = pd.DataFrame(phone_6.values.T, index=phone_6.columns, columns=phone_6['参数名'])  # 转置
    phone_7.drop(index='参数名', inplace=True)  # 删除行'参数名'
    phone_7=phone_7[col_re]
    result = result.append(phone_7) #插入手机属性到结果中
print(len(result))

result.to_csv(r"E:\pycharm\phone\phonemore_sony_Ericsson_20201029_done.csv",index=False,encoding='utf_8_sig')

'''
